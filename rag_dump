# make_chunks.py

import json

def load_schema(json_path: str):
    with open(json_path, "r") as f:
        return json.load(f)

def make_text_chunks(schema):
    chunks = []
    for col in schema:
        chunk = f"""Field: {col['field_name']}
Type: {col['field_type']}
Description: {col['description']}
Examples: {', '.join(col.get('examples', []))}"""
        chunks.append(chunk)
    return chunks

if __name__ == "__main__":
    schema = load_schema("schema.json")
    chunks = make_text_chunks(schema)

    with open("chunks.json", "w") as f:
        json.dump(chunks, f, indent=2)


# embed_chunks.py

import json
import faiss
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')  # Or another model

with open("chunks.json", "r") as f:
    chunks = json.load(f)

embeddings = model.encode(chunks, convert_to_numpy=True)

index = faiss.IndexFlatL2(embeddings[0].shape[0])
index.add(embeddings)

faiss.write_index(index, "faiss_index.idx")


# retrieve_chunks.py

import faiss
import json
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
index = faiss.read_index("faiss_index.idx")

with open("chunks.json", "r") as f:
    chunks = json.load(f)

def retrieve_chunks(query: str, top_k: int = 3):
    embedding = model.encode([query])
    D, I = index.search(embedding, top_k)
    return [chunks[i] for i in I[0]]


# generate_sql.py

from retrieve_chunks import retrieve_chunks
import openai
import os

openai.api_key = os.getenv("OPENAI_API_KEY")

def make_prompt(user_query, context_chunks):
    context = "\n---\n".join(context_chunks)
    return f"""You are a data analyst assistant.

Given the following schema context and a user question, write an SQL query.

Schema Context:
{context}

User Question:
{user_query}

SQL Query:"""

def generate_sql(user_query: str):
    context_chunks = retrieve_chunks(user_query)
    prompt = make_prompt(user_query, context_chunks)

    response = openai.ChatCompletion.create(
        model="gpt-4",  # or gpt-3.5-turbo
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message["content"]

# make_chunks.py

import json

def format_entry(entry):
    return f"""Query: {entry['query']}
Feature: {entry['feature']}
Sub-Feature: {entry['sub_feature']}"""

with open("query_data.json", "r") as f:
    data = json.load(f)

chunks = [format_entry(entry) for entry in data]

with open("query_chunks.json", "w") as f:
    json.dump(chunks, f, indent=2)


# embed_chunks.py

import json
import faiss
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")

with open("query_chunks.json", "r") as f:
    chunks = json.load(f)

embeddings = model.encode(chunks, convert_to_numpy=True)

index = faiss.IndexFlatL2(embeddings[0].shape[0])
index.add(embeddings)

faiss.write_index(index, "query_index.idx")

# retrieve_similar_queries.py

import faiss
import json
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
index = faiss.read_index("query_index.idx")

with open("query_chunks.json", "r") as f:
    chunks = json.load(f)

def retrieve_similar_queries(user_query: str, k: int = 3):
    query_emb = model.encode([user_query])
    _, I = index.search(query_emb, k)
    return [chunks[i] for i in I[0]]

# generate_sql.py

from retrieve_similar_queries import retrieve_similar_queries
import openai
import os

openai.api_key = os.getenv("OPENAI_API_KEY")

def build_prompt(user_query: str, examples: list):
    context = "\n\n".join(examples)
    return f"""
You are an analytics assistant that converts natural language queries into SQL.

Below are some example user queries and their metadata. Use them to understand the intent of the new query.

Examples:
{context}

New Query:
{user_query}

Now write an SQL query to fulfill this request.
"""

def generate_sql(user_query: str):
    examples = retrieve_similar_queries(user_query)
    prompt = build_prompt(user_query, examples)

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message["content"]


# embed_chunks.py

import json
import faiss
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel

# Path to your downloaded local MiniLM model
MODEL_PATH = "./models/all-MiniLM-L6-v2"

# Load tokenizer and model from local path
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertModel.from_pretrained(MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

def embed(texts, batch_size=16):
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        
        # Tokenize
        encoded = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).to(device)

        # Forward pass
        with torch.no_grad():
            model_output = model(**encoded)

        # Mean pooling
        token_embeddings = model_output.last_hidden_state  # (B, T, D)
        attention_mask = encoded["attention_mask"].unsqueeze(-1)  # (B, T, 1)
        masked_embeddings = token_embeddings * attention_mask
        summed = masked_embeddings.sum(dim=1)
        count = attention_mask.sum(dim=1)
        mean_pooled = summed / count

        # Normalize
        normalized = F.normalize(mean_pooled, p=2, dim=1)
        all_embeddings.append(normalized.cpu())

    return torch.cat(all_embeddings, dim=0).numpy()

# Load your chunks
with open("query_chunks.json", "r") as f:
    chunks = json.load(f)

# Generate embeddings
embeddings = embed(chunks)

# Save to FAISS
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

faiss.write_index(index, "query_index.idx")


# embed_chunks.py

import json
import faiss
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel

# Path to your downloaded local MiniLM model
MODEL_PATH = "./models/all-MiniLM-L6-v2"

# Load tokenizer and model from local path
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertModel.from_pretrained(MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

def embed(texts, batch_size=16):
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        
        # Tokenize
        encoded = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).to(device)

        # Forward pass
        with torch.no_grad():
            model_output = model(**encoded)

        # Mean pooling
        token_embeddings = model_output.last_hidden_state  # (B, T, D)
        attention_mask = encoded["attention_mask"].unsqueeze(-1)  # (B, T, 1)
        masked_embeddings = token_embeddings * attention_mask
        summed = masked_embeddings.sum(dim=1)
        count = attention_mask.sum(dim=1)
        mean_pooled = summed / count

        # Normalize
        normalized = F.normalize(mean_pooled, p=2, dim=1)
        all_embeddings.append(normalized.cpu())

    return torch.cat(all_embeddings, dim=0).numpy()

# Load your chunks
with open("query_chunks.json", "r") as f:
    chunks = json.load(f)

# Generate embeddings
embeddings = embed(chunks)

# Save to FAISS
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

faiss.write_index(index, "query_index.idx")



# retrieve_chunks.py

import json
import faiss
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel

# Path to your model and index
MODEL_PATH = "./models/all-MiniLM-L6-v2"
INDEX_PATH = "query_index.idx"
CHUNKS_PATH = "query_chunks.json"

# Load model and tokenizer
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertModel.from_pretrained(MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Load FAISS index and chunks
index = faiss.read_index(INDEX_PATH)
with open(CHUNKS_PATH, "r") as f:
    chunks = json.load(f)

# Embedding function for single query
def embed_query(text: str):
    encoded = tokenizer(
        text,
        padding=True,
        truncation=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        model_output = model(**encoded)

    token_embeddings = model_output.last_hidden_state
    attention_mask = encoded["attention_mask"].unsqueeze(-1)

    masked = token_embeddings * attention_mask
    summed = masked.sum(dim=1)
    count = attention_mask.sum(dim=1)
    mean_pooled = summed / count

    return F.normalize(mean_pooled, p=2, dim=1).cpu().numpy()

# Retrieval function
def retrieve_context(query: str, top_k=3):
    query_vector = embed_query(query)  # (1, 384)
    D, I = index.search(query_vector, top_k)
    return [chunks[i] for i in I[0]]




from retrieve_chunks import retrieve_context

query = "Weekly trend of utterances for India"
context = retrieve_context(query, top_k=3)

for i, chunk in enumerate(context):
    print(f"--- Match {i+1} ---\n{chunk}\n")
